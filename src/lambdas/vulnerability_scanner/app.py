#!/usr/bin/env python3
"""
DevSecOps Sentinel - Vulnerability Scanner Lambda Function (Production Version)
Scans for vulnerabilities in Python and Node.js dependencies using real tools.
"""

import io
import json
import logging
import os
import subprocess
import tempfile
import zipfile
from typing import Dict, List, Any, Optional

import boto3
import requests

# Add layer bin directory to PATH for Lambda layer tools
if '/opt/bin' not in os.environ.get('PATH', ''):
    os.environ['PATH'] = '/opt/bin:' + os.environ.get('PATH', '')

from sentinel_utils.utils import (
    get_github_token,
    create_session_with_retries,
    format_error_response,
    format_success_response,
    DEFAULT_TIMEOUT
)

# Configure logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# Initialize AWS clients
secrets_manager = boto3.client("secretsmanager")

# Scanner type constant
SCANNER_TYPE = "vulnerabilities"

# PROFESSIONAL VULNERABILITY SCANNING CONFIGURATION
VULNERABILITY_SOURCES = {
    "osv_api": {
        "name": "Open Source Vulnerabilities",
        "priority": 1,
        "confidence": "high",
        "python_endpoint": "https://api.osv.dev/v1/querybatch",
        "npm_endpoint": "https://api.osv.dev/v1/querybatch"
    },
    "github_advisory": {
        "name": "GitHub Security Advisory",
        "priority": 2,
        "confidence": "high",
        "endpoint": "https://api.github.com/advisories"
    },
    "snyk_api": {
        "name": "Snyk Vulnerability Database",
        "priority": 3,
        "confidence": "medium",
        "endpoint": "https://snyk.io/api/v1/test"
    },
    "nvd_api": {
        "name": "National Vulnerability Database",
        "priority": 4,
        "confidence": "high",
        "endpoint": "https://services.nvd.nist.gov/rest/json/cves/2.0"
    }
}

# Tool configurations
SCANNER_TOOLS = {
    "safety": {
        "paths": ['/opt/python/bin/safety', '/var/lang/bin/python3', 'python3'],
        "languages": ["python"],
        "confidence": "high"
    },
    "npm_audit": {
        "paths": ['/opt/bin/npm', 'npm', '/usr/bin/npm'],
        "languages": ["javascript", "nodejs"],
        "confidence": "high"
    },
    "pip_audit": {
        "paths": ['/opt/python/bin/pip-audit', 'pip-audit'],
        "languages": ["python"],
        "confidence": "high"
    }
}

# File patterns for different ecosystems
DEPENDENCY_FILES = {
    "python": ["requirements.txt", "Pipfile", "pyproject.toml", "setup.py", "poetry.lock"],
    "nodejs": ["package.json", "package-lock.json", "yarn.lock", "npm-shrinkwrap.json"],
    "java": ["pom.xml", "build.gradle", "gradle.lockfile"],
    "go": ["go.mod", "go.sum"],
    "rust": ["Cargo.toml", "Cargo.lock"],
    "ruby": ["Gemfile", "Gemfile.lock"],
    "php": ["composer.json", "composer.lock"]
}

# Individual file constants for compatibility
PYTHON_DEPS_FILE = "requirements.txt"
NODE_DEPS_FILE = "package.json"

class ProfessionalVulnerabilityOrchestrator:
    """Enterprise-grade vulnerability scanning orchestrator."""

    def __init__(self):
        self.session = create_session_with_retries()
        self.github_token = get_github_token()

    def scan_comprehensive_vulnerabilities(self, repo_path: str) -> List[Dict[str, Any]]:
        """Run comprehensive vulnerability scanning across all ecosystems and sources."""
        all_findings = []

        # Discover dependency files
        dependency_files = self._discover_dependency_files(repo_path)
        logger.info(f"Discovered dependency files: {dependency_files}")

        # Scan each ecosystem
        for ecosystem, files in dependency_files.items():
            if files:
                logger.info(f"Scanning {ecosystem} ecosystem...")
                ecosystem_findings = self._scan_ecosystem(ecosystem, files, repo_path)
                all_findings.extend(ecosystem_findings)

        # Cross-reference with multiple vulnerability sources
        enriched_findings = self._enrich_with_multiple_sources(all_findings)

        # Intelligent deduplication and prioritization
        final_findings = self._intelligent_vulnerability_deduplication(enriched_findings)

        logger.info(f"Comprehensive vulnerability scan complete:")
        logger.info(f"  - Total findings: {len(all_findings)}")
        logger.info(f"  - After enrichment: {len(enriched_findings)}")
        logger.info(f"  - After deduplication: {len(final_findings)}")

        return final_findings

    def _discover_dependency_files(self, repo_path: str) -> Dict[str, List[str]]:
        """Dynamically discover all dependency files in the repository."""
        discovered = {ecosystem: [] for ecosystem in DEPENDENCY_FILES.keys()}

        for root, dirs, files in os.walk(repo_path):
            # Skip common non-source directories
            dirs[:] = [d for d in dirs if d not in {'.git', 'node_modules', '__pycache__', '.venv', 'venv', 'target', 'build'}]

            for file in files:
                file_path = os.path.join(root, file)
                relative_path = os.path.relpath(file_path, repo_path)

                for ecosystem, patterns in DEPENDENCY_FILES.items():
                    if file in patterns:
                        discovered[ecosystem].append(relative_path)

        return {k: v for k, v in discovered.items() if v}  # Remove empty ecosystems

    def _scan_ecosystem(self, ecosystem: str, files: List[str], repo_path: str) -> List[Dict[str, Any]]:
        """Scan a specific ecosystem using multiple methods."""
        findings = []

        # Method 1: OSV API (primary)
        osv_findings = self._scan_with_osv_api(ecosystem, files, repo_path)
        findings.extend(osv_findings)

        # Method 2: GitHub Advisory API
        github_findings = self._scan_with_github_advisory(ecosystem, files, repo_path)
        findings.extend(github_findings)

        # Method 3: Tool-specific scanning (if available)
        tool_findings = self._scan_with_tools(ecosystem, files, repo_path)
        findings.extend(tool_findings)

        return findings

    def _scan_with_osv_api(self, ecosystem: str, files: List[str], repo_path: str) -> List[Dict[str, Any]]:
        """Scan using OSV API for comprehensive vulnerability data."""
        findings = []

        for file_path in files:
            full_path = os.path.join(repo_path, file_path)
            if not os.path.exists(full_path):
                continue

            try:
                dependencies = self._parse_dependencies(ecosystem, full_path)
                if dependencies:
                    osv_results = self._query_osv_api(ecosystem, dependencies)
                    for result in osv_results:
                        findings.append({
                            "source": "osv_api",
                            "ecosystem": ecosystem,
                            "file": file_path,
                            "package": result.get("package", "unknown"),
                            "version": result.get("version", "unknown"),
                            "vulnerability_id": result.get("id", "unknown"),
                            "severity": result.get("severity", "unknown"),
                            "summary": result.get("summary", ""),
                            "confidence": "high"
                        })
            except Exception as e:
                logger.warning(f"OSV API scan failed for {file_path}: {e}")

        return findings

    def _scan_with_github_advisory(self, ecosystem: str, files: List[str], repo_path: str) -> List[Dict[str, Any]]:
        """Scan using GitHub Security Advisory API."""
        findings = []

        try:
            # Query GitHub Advisory API
            headers = {"Authorization": f"token {self.github_token}"}
            response = self.session.get(
                "https://api.github.com/advisories",
                headers=headers,
                params={"ecosystem": ecosystem, "per_page": 100}
            )

            if response.status_code == 200:
                advisories = response.json()

                for file_path in files:
                    full_path = os.path.join(repo_path, file_path)
                    dependencies = self._parse_dependencies(ecosystem, full_path)

                    for dep in dependencies:
                        for advisory in advisories:
                            if self._matches_advisory(dep, advisory):
                                findings.append({
                                    "source": "github_advisory",
                                    "ecosystem": ecosystem,
                                    "file": file_path,
                                    "package": dep.get("name"),
                                    "version": dep.get("version"),
                                    "vulnerability_id": advisory.get("ghsa_id"),
                                    "severity": advisory.get("severity"),
                                    "summary": advisory.get("summary"),
                                    "confidence": "high"
                                })
        except Exception as e:
            logger.warning(f"GitHub Advisory scan failed: {e}")

        return findings

def lambda_handler(event, context):
    """
    Main Lambda handler for vulnerability scanning.
    Downloads repository and scans dependency files for vulnerabilities.
    """
    try:
        logger.info("VulnerabilityScannerFunction invoked - REAL scanning")

        # Debug mode - check layer contents
        if event.get('debug_layer'):
            return debug_layer_contents()

        # Extract repository details
        repo_details = event.get('repo_details', {})
        repo_full_name = repo_details.get('repository_full_name', '')
        commit_sha = repo_details.get('commit_sha', '')
        pr_number = repo_details.get('pr_number')

        if not repo_full_name or not commit_sha:
            return format_error_response(SCANNER_TYPE, ValueError("Repository name and commit SHA are required"))

        logger.info(f"Scanning repository: {repo_full_name}, PR: {pr_number}")

        # Download repository and scan for dependency files
        github_token = get_github_token()
        headers = {'Authorization': f'token {github_token}'}
        zip_url = f"https://api.github.com/repos/{repo_full_name}/zipball/{commit_sha}"

        all_findings = scan_repository_dependencies(zip_url, headers)

        logger.info(f"Vulnerability scan completed. Found {len(all_findings)} total vulnerabilities.")

        return format_success_response(SCANNER_TYPE, all_findings)

    except requests.exceptions.HTTPError as e:
        logger.error(f"GitHub API HTTP error: {e.response.status_code} - {e.response.text}", exc_info=True)
        return format_error_response(
            SCANNER_TYPE,
            Exception(f"GitHub API error: {e.response.status_code}")
        )
    except Exception as e:
        logger.error(f"Error in vulnerability scanner: {e}", exc_info=True)
        return format_error_response(SCANNER_TYPE, e)

def debug_layer_contents():
    """Debug function to check what's available in the layer"""
    import subprocess

    results = {
        "paths_checked": [],
        "files_found": [],
        "environment": {},
        "tool_checks": [],
        "tool_tests": []
    }

    # Check environment variables
    results["environment"]["PATH"] = os.environ.get("PATH", "")
    results["environment"]["PYTHONPATH"] = os.environ.get("PYTHONPATH", "")

    # Check common paths
    paths_to_check = ["/opt", "/opt/bin", "/opt/python", "/var/lang/bin", "/usr/bin"]

    for path in paths_to_check:
        results["paths_checked"].append(path)
        if os.path.exists(path):
            try:
                contents = os.listdir(path)
                results["files_found"].append({"path": path, "contents": contents})
            except Exception as e:
                results["files_found"].append({"path": path, "error": str(e)})

    # Check specific tools
    tools_to_check = ["/opt/bin/safety", "/opt/bin/npm", "safety", "npm"]

    for tool in tools_to_check:
        tool_result = {"tool": tool}

        if os.path.exists(tool):
            tool_result["exists"] = True
            tool_result["executable"] = os.access(tool, os.X_OK)
        else:
            tool_result["exists"] = False

        try:
            result = subprocess.run(["which", tool], capture_output=True, text=True)
            tool_result["which_result"] = result.stdout.strip() if result.returncode == 0 else "not found"
        except Exception as e:
            tool_result["which_error"] = str(e)

        results["tool_checks"].append(tool_result)

    # Test actual tool execution
    python_cmd = '/var/lang/bin/python3'
    env = os.environ.copy()
    env['PYTHONPATH'] = '/opt/python:' + env.get('PYTHONPATH', '')

    # Test pip-audit
    try:
        cmd = [python_cmd, '-m', 'pip_audit', '--version']
        result = subprocess.run(cmd, capture_output=True, text=True, env=env, timeout=10)
        results["tool_tests"].append({
            "tool": "pip_audit",
            "command": " ".join(cmd),
            "return_code": result.returncode,
            "stdout": result.stdout[:200],
            "stderr": result.stderr[:200]
        })
    except Exception as e:
        results["tool_tests"].append({
            "tool": "pip_audit",
            "error": str(e)
        })

    # Test safety
    try:
        cmd = [python_cmd, '-m', 'safety', '--version']
        result = subprocess.run(cmd, capture_output=True, text=True, env=env, timeout=10)
        results["tool_tests"].append({
            "tool": "safety",
            "command": " ".join(cmd),
            "return_code": result.returncode,
            "stdout": result.stdout[:200],
            "stderr": result.stderr[:200]
        })
    except Exception as e:
        results["tool_tests"].append({
            "tool": "safety",
            "error": str(e)
        })

    return {
        "statusCode": 200,
        "scanner_type": "debug",
        "debug_results": results
    }

def find_tool(tool_paths: List[str]) -> Optional[str]:
    """Find the first available tool from a list of paths."""
    for tool_path in tool_paths:
        # Check if absolute path exists and is executable
        if os.path.exists(tool_path) and os.access(tool_path, os.X_OK):
            return tool_path

        # For relative paths, check in PATH
        if not tool_path.startswith('/'):
            # Check in common directories
            for path_dir in ['/opt/bin', '/usr/bin', '/bin', '/var/lang/bin']:
                full_path = os.path.join(path_dir, tool_path)
                if os.path.exists(full_path) and os.access(full_path, os.X_OK):
                    return full_path

    return None

def scan_repository_dependencies(zip_url: str, headers: Dict[str, str]) -> List[Dict[str, Any]]:
    """
    Download repository zipball and scan dependency files for vulnerabilities.

    Args:
        zip_url: GitHub API URL for repository zipball
        headers: HTTP headers including GitHub token

    Returns:
        List of vulnerability findings
    """
    all_findings = []

    with tempfile.TemporaryDirectory() as temp_dir:
        # Download repository zipball
        session = create_session_with_retries()
        response = session.get(zip_url, headers=headers, timeout=DEFAULT_TIMEOUT)
        response.raise_for_status()

        # Extract zipball
        with zipfile.ZipFile(io.BytesIO(response.content)) as zip_file:
            zip_file.extractall(temp_dir)

        # Find the extracted directory (GitHub creates a directory with repo name and commit)
        extracted_dirs = [d for d in os.listdir(temp_dir) if os.path.isdir(os.path.join(temp_dir, d))]
        if not extracted_dirs:
            logger.warning("No directories found in extracted zipball")
            return all_findings

        repo_dir = os.path.join(temp_dir, extracted_dirs[0])
        logger.info(f"Scanning repository directory: {repo_dir}")

        # Find and scan dependency files
        dependency_files = find_dependency_files(repo_dir)
        logger.info(f"Found {len(dependency_files)} dependency files total")

        for file_path, file_type in dependency_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                relative_path = os.path.relpath(file_path, repo_dir)
                logger.info(f"Processing {file_type} file: {relative_path} (size: {len(content)} chars)")

                if file_type == 'python':
                    logger.info(f"Scanning Python dependencies from: {relative_path}")
                    logger.info(f"Content preview: {content[:200]}...")
                    findings = scan_python_dependencies(content, relative_path)
                    logger.info(f"Python scan found {len(findings)} vulnerabilities")
                    all_findings.extend(findings)
                elif file_type == 'nodejs':
                    logger.info(f"Scanning Node.js dependencies from: {relative_path}")
                    logger.info(f"Content preview: {content[:200]}...")
                    findings = scan_node_dependencies(content, relative_path)
                    logger.info(f"Node.js scan found {len(findings)} vulnerabilities")
                    all_findings.extend(findings)

            except Exception as e:
                logger.error(f"Error scanning {file_path}: {e}")
                continue

    return all_findings

def find_dependency_files(repo_dir: str) -> List[tuple]:
    """
    Find dependency files in the repository.

    Args:
        repo_dir: Repository directory path

    Returns:
        List of tuples (file_path, file_type)
    """
    dependency_files = []

    for root, dirs, files in os.walk(repo_dir):
        for file in files:
            file_path = os.path.join(root, file)

            if file == PYTHON_DEPS_FILE:
                dependency_files.append((file_path, 'python'))
            elif file == NODE_DEPS_FILE:
                dependency_files.append((file_path, 'nodejs'))

    return dependency_files

def process_fixed_version(fixed_version: str, package_name: str, current_version: str, ecosystem: str) -> str:
    """
    Process fixed version to make it more user-friendly.
    Converts commit SHAs to semantic versions or user-friendly messages.
    """
    if not fixed_version or fixed_version == "unknown":
        # No fixed version available
        return "check for updates"
    
    # Check if it looks like a commit SHA (40 hex characters)
    if len(fixed_version) == 40 and all(c in '0123456789abcdef' for c in fixed_version.lower()):
        # It's a commit SHA
        # For common packages, we can suggest checking latest versions
        if ecosystem == "npm":
            return "check npm for latest"
        elif ecosystem == "PyPI":
            return "check PyPI for latest"
        else:
            return "update to latest"
    
    # Check if it's a short SHA (7-12 characters)
    if 7 <= len(fixed_version) <= 12 and all(c in '0123456789abcdef' for c in fixed_version.lower()):
        return "update to latest"
    
    # If it looks like a version number, return it as-is
    if any(char in fixed_version for char in ['.', '-', '+']) or fixed_version[0].isdigit():
        return fixed_version
    
    # Default fallback
    return fixed_version

def scan_python_dependencies(requirements_content: str, file_path: str) -> List[Dict[str, Any]]:
    """
    Scan Python dependencies using OSV (Open Source Vulnerabilities) API.
    """
    findings = []

    if not requirements_content.strip():
        return findings

    logger.info(f"Scanning Python dependencies using OSV API")

    # Parse requirements.txt
    packages = []
    for line in requirements_content.strip().split('\n'):
        line = line.strip()
        if not line or line.startswith('#'):
            continue

        # Parse package==version format
        if '==' in line:
            package_name, version = line.split('==', 1)
            packages.append({
                'name': package_name.strip(),
                'version': version.strip()
            })

    logger.info(f"Found {len(packages)} Python packages to scan")

    # Query OSV API for each package
    session = create_session_with_retries()

    for package in packages:
        try:
            # Query OSV API
            osv_query = {
                "package": {
                    "name": package['name'],
                    "ecosystem": "PyPI"
                },
                "version": package['version']
            }

            response = session.post(
                'https://api.osv.dev/v1/query',
                json=osv_query,
                timeout=10
            )

            if response.status_code == 200:
                osv_data = response.json()
                vulns = osv_data.get('vulns', [])

                for vuln in vulns:
                    # Extract fixed version from affected ranges
                    fixed_version = None
                    affected = vuln.get('affected', [])
                    for aff in affected:
                        if aff.get('package', {}).get('ecosystem') == 'PyPI':
                            ranges = aff.get('ranges', [])
                            for range_info in ranges:
                                events = range_info.get('events', [])
                                for event in events:
                                    if 'fixed' in event:
                                        fixed_version = event['fixed']
                                        break
                                if fixed_version:
                                    break
                    
                    # Extract severity from database_specific or severity field
                    severity = "UNKNOWN"
                    if 'severity' in vuln:
                        severity_data = vuln['severity']
                        if isinstance(severity_data, list) and severity_data:
                            severity = severity_data[0].get('score', 'UNKNOWN')
                        elif isinstance(severity_data, str):
                            severity = severity_data
                    elif 'database_specific' in vuln and 'severity' in vuln['database_specific']:
                        severity = vuln['database_specific']['severity']
                    
                    # Map severity scores to levels
                    if isinstance(severity, str):
                        if severity.upper() in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']:
                            severity = severity.upper()
                        else:
                            severity = 'HIGH'  # Default to HIGH if unknown
                    
                    finding = {
                        "type": "vulnerability",
                        "language": "python",
                        "package": package['name'],
                        "version": package['version'],
                        "fixed_version": process_fixed_version(fixed_version, package['name'], package['version'], "PyPI"),
                        "severity": severity,
                        "description": vuln.get('summary', 'Vulnerability found'),
                        "vulnerability_id": vuln.get('id', 'UNKNOWN'),
                        "file": file_path
                    }
                    findings.append(finding)
                    logger.info(f"Found vulnerability: {package['name']} {package['version']} - {vuln.get('id')}")

        except Exception as e:
            logger.error(f"Error querying OSV for {package['name']}: {e}")
            continue

    logger.info(f"OSV scan found {len(findings)} Python vulnerabilities")
    return findings

def scan_node_dependencies(package_json_content: str, file_path: str) -> List[Dict[str, Any]]:
    """
    Scan Node.js dependencies using OSV (Open Source Vulnerabilities) API.
    """
    findings = []

    if not package_json_content.strip():
        return findings

    logger.info(f"Scanning Node.js dependencies using OSV API")

    try:
        # Parse package.json
        package_data = json.loads(package_json_content)

        # Get all dependencies
        packages = []
        all_deps = {}
        all_deps.update(package_data.get('dependencies', {}))
        all_deps.update(package_data.get('devDependencies', {}))

        for package_name, version in all_deps.items():
            # Clean version (remove ^ ~ etc.)
            clean_version = version.lstrip('^~>=<')
            packages.append({
                'name': package_name,
                'version': clean_version
            })

        logger.info(f"Found {len(packages)} Node.js packages to scan")

        # Query OSV API for each package
        session = create_session_with_retries()

        for package in packages:
            try:
                # Query OSV API
                osv_query = {
                    "package": {
                        "name": package['name'],
                        "ecosystem": "npm"
                    },
                    "version": package['version']
                }

                response = session.post(
                    'https://api.osv.dev/v1/query',
                    json=osv_query,
                    timeout=10
                )

                if response.status_code == 200:
                    osv_data = response.json()
                    vulns = osv_data.get('vulns', [])

                    for vuln in vulns:
                        # Extract fixed version from affected ranges
                        fixed_version = None
                        affected = vuln.get('affected', [])
                        for aff in affected:
                            if aff.get('package', {}).get('ecosystem') == 'npm':
                                ranges = aff.get('ranges', [])
                                for range_info in ranges:
                                    events = range_info.get('events', [])
                                    for event in events:
                                        if 'fixed' in event:
                                            fixed_version = event['fixed']
                                            break
                                    if fixed_version:
                                        break
                                if fixed_version:
                                    break
                        
                        # Extract severity from database_specific or severity field
                        severity = "UNKNOWN"
                        if 'severity' in vuln:
                            severity_data = vuln['severity']
                            if isinstance(severity_data, list) and severity_data:
                                severity = severity_data[0].get('score', 'UNKNOWN')
                            elif isinstance(severity_data, str):
                                severity = severity_data
                        elif 'database_specific' in vuln and 'severity' in vuln['database_specific']:
                            severity = vuln['database_specific']['severity']
                        
                        # Map severity scores to levels
                        if isinstance(severity, str):
                            if severity.upper() in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']:
                                severity = severity.upper()
                            else:
                                severity = 'HIGH'  # Default to HIGH if unknown
                        
                        finding = {
                            "type": "vulnerability",
                            "language": "nodejs",
                            "package": package['name'],
                            "version": package['version'],
                            "fixed_version": process_fixed_version(fixed_version, package['name'], package['version'], "npm"),
                            "severity": severity,
                            "description": vuln.get('summary', 'Vulnerability found'),
                            "vulnerability_id": vuln.get('id', 'UNKNOWN'),
                            "file": file_path
                        }
                        findings.append(finding)
                        logger.info(f"Found vulnerability: {package['name']} {package['version']} - {vuln.get('id')}")

            except Exception as e:
                logger.error(f"Error querying OSV for {package['name']}: {e}")
                continue

        logger.info(f"OSV scan found {len(findings)} Node.js vulnerabilities")

    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse package.json: {e}")

    return findings
